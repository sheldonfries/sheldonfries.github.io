<!DOCTYPE html>
<html>
  <head>
    <link rel="stylesheet" href="/css/blog.css">
    <link rel="stylesheet" href="/css/bootstrap.min.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<link rel="stylesheet" href="/css/styles.css">
<link href="https://fonts.googleapis.com/css?family=Karla|Ubuntu" rel="stylesheet"> 
<script src="/js/jquery-3.3.1.min.js"></script>
<script src="/js/popper.min.js"></script>
<script src="/js/bootstrap.min.js"></script>
<title>Sheldon Fries</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  </head>
  <body>
      <nav class="navbar navbar-dark navbar-expand-md bg-dark fixed-top pr-5">
    <a class="navbar-brand" href="/">sheldonfries</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="ml-auto navbar-nav">
            <!-- <a class="nav-item nav-link" href="/apps">Apps</a> -->
            <li class="dropdown nav-item">
                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Research<span class="caret"></span></a>
                <div class="dropdown-menu">
                    <a class="dropdown-item" href="/blog/Detecting-Tweet-Sentiment-Ordinal_Classification/">Detecting Tweet Sentiment</a>
                    <a class="dropdown-item" href="/blog/C-TIP-Total-Infected-Prediction-COVID-19/">COVID-19 SIR Projection</a>
                </div>
            </li>
            <li class="dropdown nav-item">
                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Projects<span class="caret"></span></a>
                <div class="dropdown-menu">
                    <span class="dropdown-header">Android Apps</span>
                    <a class="dropdown-item" href="https://play.google.com/store/apps/details?id=cc.cu.hockeycalculator.goofspiel&hl=en">Goofspiel</a>
                    <a class="dropdown-item" href="https://play.google.com/store/apps/details?id=cc.cu.hockeycalculator.hiittimer&hl=en">HIIT Timer</a>
                    <a class="dropdown-item" href="https://play.google.com/store/apps/details?id=com.companyname.Jumpy_Cat">Jumpy Cat</a>
                    <span class="dropdown-header">Web Apps</span>
                    <a class="dropdown-item" href="https://sheldonfries.com/transit">Transit</a>
                    <a class="dropdown-item" href="http://hockey.sheldonfries.com">Hockey Calculator</a>
                    <span class="dropdown-header">Miscellaneous</span>
                    <a class="dropdown-item" href="https://github.com/sheldonfries/TOSDataPuller">TOSDataPuller</a>
                    <a class="dropdown-item" href="https://github.com/sheldonfries/nhl-study">NHL Player Study</a>
                </div>
            </li>
            <a class="nav-item nav-link btn btn-outline-primary" href="/assets/Resume.pdf" target="_blank" role="button">Resume</a>
            <!-- <a class="nav-item nav-link" href="/blog">Blog</a> -->
        </ul>
    </div>
</nav>

      <div class="container">
        <div class="post-container">
          <article class="post">
  <nav aria-label="breadcrumb" class="breadcrumb-container">
    <ol class="breadcrumb">
      <li class="breadcrumb-item"><a href="/blog">Blog</a></li>
      <li class="breadcrumb-item active" aria-current="page">Detecting Tweet Sentiment by Ordinal Classification</li>
    </ol>
  </nav>
  <h1>Detecting Tweet Sentiment by Ordinal Classification</h1>

  <div class="date">
    December  8, 2019
  </div>

  <div class="blog-image">
    <img src="https://pixy.org/src2/576/5760487.jpg">
  </div>

  <div class="entry">
    <p>Sheldon Fries, Heidi Tong, Yi-Hsuan Lo</p>

<h1 id="motivation">Motivation</h1>
<p>Our group has chosen to do an ordinal classification task on the sentiment of a corpus of tweets. We will be classifying tweets by assigning each tweet a score based on their level of positive or negative sentiment. Each class is represented by a number between 3 and -3, in which a score of 3 indicates that the tweet is extremely positive, while a score of -3 indicates that a tweet is extremely negative. We are focusing on improving upon the baseline implementation of the model used in <a href="https://competitions.codalab.org/competitions/17751#learn_the_details-overview">SemEval-2018 Task 1</a>, which has an accuracy of just over 50%.</p>

<p>Like with other forms of online communication, it can still be challenging at times for even a person to decipher the sentiment of tweets. This challenge raises questions regarding what exactly is required for accurate interpretation of social media. Could a computer do better in performing this task? We would like to analyze existing algorithms while also seeing if we are capable of finding a better way to categorize tweet sentiment.</p>

<hr />

<h1 id="approach">Approach</h1>
<p>To start this project, we experimented with several classifiers using <a href="https://scikit-learn.org/stable/">scikit-learn</a> and <a href="https://www.nltk.org/">Natural Language Toolkit (nltk)</a>. The classifiers we tested include Bernoulli Naive Bayes, Gaussian Naive Bayes, Multinomial Bayes, Logistic Regression, SVM, Linear SVC, and Random Forest. These Python packages also allowed us to experiment with the Bag of Words algorithm using its CountVectorizer and the tf-idf algorithm using its TfidfVectorizer. After adjusting the classifier parameters in an attempt to get their best performing versions, we tried to further improve upon their performance through data preprocessing and an ensemble classification algorithm found <a href="https://www.sciencedirect.com/science/article/pii/S187705091830841X">here</a>.</p>

<h3 id="classifiers">Classifiers</h3>

<h3 id="naive-bayes">Naive Bayes</h3>

<p>Naive Bayes classifiers assume that each feature in our data is conditionally independent. The Naive Bayes model is a generative model that predicts the probability <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html" title="In Depth: Naive Bayes Classification">given the joint distribution of the feature and target</a>.</p>

<p>We analyzed results from the following three Naive Bayes Classifiers:</p>

<ul>
  <li>Bernoulli Naive Bayes
    <ul>
      <li>This version of Naive Bayes assumes that all features are binary. In other words, a word either exists in our document or does not.</li>
    </ul>
  </li>
  <li>Gaussian Naive Bayes
    <ul>
      <li>This version of Naive Bayes assumes that our data is drawn from a Gaussian distribution, with no covariance between any of our labels.</li>
    </ul>
  </li>
  <li>Multinomial Naive Bayes
    <ul>
      <li>This version of Naive Bayes takes frequency into account and assumes that our data is drawn from a multinomial distribution.</li>
    </ul>
  </li>
</ul>

<h3 id="logistic-regression">Logistic Regression</h3>

<p>The Logistic regression model is a <a href="https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c" title="Naive Bayes vs Logistic Regression">discriminative model that learns the input to output mapping in order to model probability</a>. It attempts to find the boundary that <a href="https://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/" title="Comparison between Naïve Bayes and Logistic Regression">best separates each classification</a> and tries to minimize error. Unlike with Naive Bayes classifiers, this model does not assume feature independence.</p>

<h3 id="support-vector-machines-and-linear-support-vector-classification">Support Vector Machines and Linear Support Vector Classification</h3>

<p>By definition, SVM does not support comparisons between more than two classes. To account for this, an “instance” of SVM will compare two classes in a one-vs-one scheme, and repeat this for all pairs of classes until each class has been compared against all of the others. At this point, the results are combined.</p>

<p>The training data is interpreted by the SVM as a collection of n vectors, each of dimension m. The hyperplanes which divide these vectors are (m-1)-dimensional (for example, a 2-dimensional plane would be divided by hyperplane lines), and serve as dividers between each class. Hyperplanes are calculated <a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf#page=153" title="The Elements of Statistical Learning">by maximizing the margin between the data points</a>.</p>

<p>In addition to trying a standard SVM model, we also attempted to use the Linear SVC classifier provided by sklearn. This model uses the same algorithm as SVM but uses a linear kernel, which may perform faster.</p>

<h3 id="random-forest">Random Forest</h3>

<p>Unlike with Logistic Regression, the Random Forest algorithm <a href="https://towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4" title="Is Random Forest better than Logistic Regression? (a comparison)">does not assume a linear relationship from the data</a>. This model considers certain features to be more important than others and makes use of ensemble learning by producing multiple decision trees using random sampling and averaging out the leaf nodes. The accuracy of this model relies heavily on the number of decision trees used. Thus, this will be a parameter to further analyze in a latter section.</p>

<h3 id="ensemble-classifier">Ensemble Classifier</h3>

<p><a href="https://www.sciencedirect.com/science/article/pii/S187705091830841X">Ankit (2018)</a> proposes a weighted ensemble classifier as an approach to tweet sentiment analysis. They provide an algorithm which builds on the results of several classifiers in order to form a classifier that is more “robust”. Each classifier is assigned a weight which is then used to calculate the overall positive or negative score of a tweet. Their algorithm only accounts for two classes (positive or negative), so we had to make adjustments to fit our seven class system.</p>

<h2 id="feature-extraction-algorithms">Feature Extraction Algorithms</h2>

<h3 id="bag-of-words">Bag of Words</h3>
<p>Bag of Words is a model that considers how common individual words are in a dataset. The model works as expected based on the name: it works much like blindly reaching into a bag and pulling out words. Words that appear most frequently are the words that are most likely to be chosen; context such as grammar is not a consideration. This is useful for determining how frequently certain words appear in the data, allowing for general themes to be extracted from the data based on its most common words.</p>

<h3 id="tf-idf">tf-idf</h3>
<p>Given that words like “a”, “the”, and “it” will appear very frequently in almost any context, it is important to consider how this might affect the results from Bag of Words. Term Frequency - Inverse Document Frequency, or tf-idf for short, handles both the frequency count and any adjustment for words that are deemed unimportant. The frequency count is calculated by counting the number of occurrences of a word in the data, and the inverse document frequency calculates how frequently it appears across the entire dataset. If a word appears frequently in one tweet but very rarely across the entire dataset, it will score highly. If a word either appears infrequently, or appears very frequently across the entire dataset, it will have a lower score.</p>

<hr />

<h1 id="data">Data</h1>
<p>The data files that were used for this project are from <a href="https://competitions.codalab.org/competitions/17751#learn_the_details-overview">SemEval-2018 Task 1: Affect in tweets</a>. More specifically, the fourth subtask data file which contained the sentiment scores of tweets was used for training and testing our model. The exact data (training, development, and test) used in this project can be found <a href="http://saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/SemEval2018-Task1-all-data.zip">here</a>.</p>

<p>No other external data files that were not provided to us were used in this project.</p>

<hr />

<h1 id="code">Code</h1>
<p>We used the scikit-learn and Natural Language Toolkit (nltk) packages to experiment with various algorithms and machine learning models. The code below includes the portions of our program which depend on these packages.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span><span class="p">,</span> <span class="n">WordNetLemmatizer</span> 

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span><span class="p">,</span> <span class="n">svm</span><span class="p">,</span> <span class="n">linear_model</span><span class="p">,</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span><span class="p">,</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">resample</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span><span class="p">,</span> <span class="n">MultinomialNB</span><span class="p">,</span> <span class="n">BernoulliNB</span><span class="p">,</span> <span class="n">ComplementNB</span>


<span class="k">def</span> <span class="nf">process_tweets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stopwords_file</span><span class="p">):</span>

    <span class="c"># get stop words</span>
    <span class="n">swfile</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">stopwords_file</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
    <span class="n">stopwords</span> <span class="o">=</span> <span class="n">swfile</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">swfile</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="c"># remove \n from each stop word</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">:</span>
        <span class="n">stopwords</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span><span class="s">''</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="c"># 4. Stemming</span>
    <span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
        <span class="n">newWord</span> <span class="o">=</span> <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="n">tweet</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">newWord</span><span class="p">)</span>
        
<span class="k">def</span> <span class="nf">ensembleClassify</span><span class="p">():</span>

    <span class="c"># Our four best performing classifiers were: Linear SVC, SVM, MutlinomialNB, and Logistic Regression</span>
    <span class="c"># so we take these classifiers and build our ensemble classifier.</span>

    <span class="c"># Multinomial NB </span>
    <span class="n">MultiNB</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
    <span class="n">MultiNB</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">train_valence</span><span class="p">)</span>
    <span class="n">MultiNB_results</span> <span class="o">=</span> <span class="n">MultiNB</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
    <span class="n">MultiNB_score</span> <span class="o">=</span> <span class="n">MultiNB</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">test_valence</span><span class="p">)</span>
    
    <span class="c"># Logistic Regression</span>
    <span class="n">logreg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'lbfgs'</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s">'multinomial'</span><span class="p">)</span>
    <span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">train_valence</span><span class="p">)</span>
    <span class="n">logreg_results</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
    <span class="n">logreg_score</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">test_valence</span><span class="p">)</span>

    <span class="c"># Linear SVC</span>
    <span class="n">SVC</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">SVC</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">train_valence</span><span class="p">)</span>
    <span class="n">SVC_results</span> <span class="o">=</span> <span class="n">SVC</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
    <span class="n">SVC_score</span> <span class="o">=</span> <span class="n">SVC</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">test_valence</span><span class="p">)</span>

    <span class="c"># SVM </span>
    <span class="n">SVM</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s">'auto'</span><span class="p">)</span>
    <span class="n">SVM</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">train_valence</span><span class="p">)</span>
    <span class="n">SVM_results</span> <span class="o">=</span> <span class="n">SVM</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
    <span class="n">SVM_score</span> <span class="o">=</span> <span class="n">SVM</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">testX</span><span class="p">,</span> <span class="n">test_valence</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">re_sample</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
        <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">resample</span><span class="p">(</span><span class="n">tweets</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">],</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">downsample_zero</span><span class="p">():</span>
    <span class="n">train_zero_downsampled</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">train_zeros</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
        
<span class="n">count</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s">'english'</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">bag_of_words</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_tweets</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">bag_of_words</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">bag_of_words2</span> <span class="o">=</span> <span class="n">count</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_tweets</span><span class="p">)</span>
<span class="n">testX</span> <span class="o">=</span> <span class="n">bag_of_words2</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>

<span class="c">## Classifiers we tried but do not give us our highest score.</span>
<span class="c">## Uncomment one of the clf to test another classifier.</span>
<span class="c">## Make sure to comment out the svc classifier below!</span>
<span class="c">#clf = BernoulliNB() </span>
<span class="c">#clf = GaussianNB()</span>
<span class="c">#clf = MultinomialNB()</span>
<span class="c">#clf = RandomForestClassifier(n_estima tors = 200)</span>
<span class="c">#clf = linear_model.LogisticRegression(random_state=0, C=0.5, solver='lbfgs', multi_class='multinomial')</span>
<span class="c">#clf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')</span>
<span class="c">#clf.fit(X, train_valence)</span>
<span class="c">#results = clf.predict(testX)</span>
    
<span class="c"># Our best performing classifier:</span>
<span class="n">SVC</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">SVC</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">train_valence</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">SVC</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testX</span><span class="p">)</span>
</code></pre></div></div>

<p>The following code is found in each of the homework assignments. For our project, it was used to allow arguments for different training and test sets, as well as custom stop words.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optparser</span> <span class="o">=</span> <span class="n">optparse</span><span class="o">.</span><span class="n">OptionParser</span><span class="p">()</span>

<span class="n">optparser</span><span class="o">.</span><span class="n">add_option</span><span class="p">(</span><span class="s">"-c"</span><span class="p">,</span> <span class="s">"--trainingdata"</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s">'train'</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="s">'train.txt'</span><span class="p">),</span> <span class="n">help</span><span class="o">=</span><span class="s">"training data"</span><span class="p">)</span>
<span class="n">optparser</span><span class="o">.</span><span class="n">add_option</span><span class="p">(</span><span class="s">"-i"</span><span class="p">,</span> <span class="s">"--inputfile"</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s">"input"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'data'</span><span class="p">,</span> <span class="s">'test.txt'</span><span class="p">),</span> <span class="n">help</span><span class="o">=</span><span class="s">"file to analyze"</span><span class="p">)</span>
<span class="n">optparser</span><span class="o">.</span><span class="n">add_option</span><span class="p">(</span><span class="s">"-l"</span><span class="p">,</span> <span class="s">"--logfile"</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s">"logfile"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"log file for debugging"</span><span class="p">)</span>
<span class="n">optparser</span><span class="o">.</span><span class="n">add_option</span><span class="p">(</span><span class="s">"-m"</span><span class="p">,</span> <span class="s">"--model"</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s">"model"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s">"NB Classifier model"</span><span class="p">)</span>
<span class="n">optparser</span><span class="o">.</span><span class="n">add_option</span><span class="p">(</span><span class="s">"-s"</span><span class="p">,</span> <span class="s">"--stop"</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s">"stop"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'stopwords.txt'</span><span class="p">),</span> <span class="n">help</span><span class="o">=</span><span class="s">"List of stop words"</span><span class="p">)</span>

<span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">optparser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</code></pre></div></div>

<p>Additionally, the demoji package was used to replace emojis in tweets with their matching description.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">demoji</span>

<span class="n">demoji</span><span class="o">.</span><span class="n">download_codes</span><span class="p">()</span>

<span class="n">emoji_list</span> <span class="o">=</span> <span class="n">demoji</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="n">tweet</span><span class="p">)</span> <span class="c"># remove emojis</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">emoji_list</span><span class="p">:</span>
    <span class="n">tweet</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">u'</span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">key</span><span class="p">,</span> <span class="s">u'</span><span class="si">%</span><span class="s">s'</span> <span class="o">%</span> <span class="n">emoji_list</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">tweet</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h1 id="experimental-setup">Experimental Setup</h1>
<p>The data set will be evaluated by calculating the Pearson correlation coefficient between the generated ratings and their matching labels. We chose this form of evaluation so that we could more easily compare with the baseline system provided for SemEval-2018 Task 1 and WASSA-2017 Shared Task on Emotion Intensity.</p>

<p>Our experiment consists of the following components:</p>

<h3 id="classifiers-1">Classifiers</h3>

<p>We first did a general comparison of classifiers using their default settings in the scikit-learn library and the bag of words algorithm for feature extraction. Once again, these classifiers include: Gaussian Naive Bayes, Multinomial Naive Bayes, Bernoulli Naive Bayes, Logistic Regression, SVM, Linear SVC and Random Forest.</p>

<p>We then took the four best performing classifiers from the list and used them to build an ensemble classifier.</p>

<h3 id="classifier-parameters">Classifier Parameters</h3>

<p>Each individual classifier contained multiple parameter options that could be tweaked to further improve performance. The parameters are listed for each classifier below.</p>

<ul>
  <li>Logistic Regression
    <ul>
      <li>random_state: Seed for the pseudo-random number generator.</li>
      <li>C: Regularization strength.</li>
      <li>multi_class: How to fit the data.</li>
      <li>dual: Whether to use dual or primal formulation.</li>
    </ul>
  </li>
  <li>Support Vector Machines
    <ul>
      <li>kernel: Type of kernel used.</li>
      <li>gamma: Coefficient for the kernel.</li>
    </ul>
  </li>
  <li>Linear Support Vector Classification
    <ul>
      <li>random_state: Seed for the pseudo-random number generator.</li>
      <li>dual: Whether to use dual or primal formulation.</li>
      <li>tol: Stopping criteria.</li>
    </ul>
  </li>
  <li>Random Forest
    <ul>
      <li>n_estimators: Number of trees in the forest.</li>
    </ul>
  </li>
</ul>

<h3 id="feature-extraction">Feature Extraction</h3>

<p>Taking the best performing parameters for each classifier, we then compared different methods to extract features from our data by comparing the results from using the bag of words algorithm and using the tf-idf algorithm. We considered both methods for each of our chosen classifiers to identify the best combination.</p>

<h3 id="data-processing">Data Processing</h3>

<p>Taking our best performing classifier, we tested, combined, and compared the results of different data processing techniques in an attempt to increase our accuracy score. The data processing techniques we compared include:</p>

<ul>
  <li>Stemming vs. Lemmatization vs. No Stemming or Lemmatization</li>
  <li>Removing Emojis vs. Replacing Emojis With Words vs. Not Removing Emojis</li>
  <li>Using Stop Words vs. Not Using Stop Words</li>
  <li>Replacing Slang Words vs. Not Replacing Slang Words</li>
  <li>Upsampled/Downsampled Classes vs. Regularly Sampled Classes</li>
</ul>

<hr />

<h1 id="results">Results</h1>

<p>Table 1: Comparing Classifiers (default options, bag of words)</p>

<table>
  <thead>
    <tr>
      <th><strong>Classifier</strong></th>
      <th>Gaussian Naive Bayes</th>
      <th>Multinomial Naive Bayes</th>
      <th>Bernoulli Naive Bayes</th>
      <th>Logistic Regression</th>
      <th>Support Vector Machine</th>
      <th>Linear SVC</th>
      <th>Random Forest</th>
      <th>Ensemble Classifier</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Score</strong></td>
      <td>0.4443</td>
      <td>0.5718</td>
      <td>0.0704</td>
      <td>0.5818</td>
      <td>0.6013</td>
      <td><strong>0.6107</strong></td>
      <td>0.4919</td>
      <td>0.5926</td>
    </tr>
  </tbody>
</table>

<h3 id="comparing-classifier-parameter-options">Comparing Classifier Parameter Options</h3>

<p>Table 2.1: Gaussian Naive Bayes</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Default</td>
      <td style="text-align: center">0.4443</td>
    </tr>
    <tr>
      <td style="text-align: center">Priors = None, var_smoothing = 1e-08</td>
      <td style="text-align: center">0.4443</td>
    </tr>
    <tr>
      <td style="text-align: center">Priors = None, var_smoothing = 1e-07</td>
      <td style="text-align: center">0.4443</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Priors = None, var_smoothing = 1e-05</strong></td>
      <td style="text-align: center"><strong>0.4448</strong></td>
    </tr>
  </tbody>
</table>

<p>Table 2.2: Multinomial Naive Bayes</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Default</td>
      <td style="text-align: center">0.5718</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>alpha = 1.2, fit_prior = True</strong></td>
      <td style="text-align: center"><strong>0.5755</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">alpha = 1.2, fit_prior = False</td>
      <td style="text-align: center">0.5738</td>
    </tr>
    <tr>
      <td style="text-align: center">alpha = 1.5, fit_prior = True</td>
      <td style="text-align: center">0.5734</td>
    </tr>
  </tbody>
</table>

<p>Table 2.3: Bernoulli Naive Bayes</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Default</td>
      <td style="text-align: center">0.0704</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>alpha = 1.2, fit_prior = True</strong></td>
      <td style="text-align: center"><strong>0.0739</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">alpha = 1.2, fit_prior = False</td>
      <td style="text-align: center">0.0739</td>
    </tr>
  </tbody>
</table>

<p>Table 2.4: Logistic Regression</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Default</strong></td>
      <td style="text-align: center"><strong>0.5818</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">random_state = 0, C = 0.5, multi_class = multinomial</td>
      <td style="text-align: center">0.5781</td>
    </tr>
    <tr>
      <td style="text-align: center">random_state = 0, C = 0.75, dual = False</td>
      <td style="text-align: center">0.5794</td>
    </tr>
    <tr>
      <td style="text-align: center">random_state = 0, C = 0.75, dual = False, multi_class = ovr</td>
      <td style="text-align: center">0.5809</td>
    </tr>
  </tbody>
</table>

<p>Table 2.5: Support Vector Machine</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Default</strong></td>
      <td style="text-align: center"><strong>0.6013</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">kernel = rbf, gamma = scale</td>
      <td style="text-align: center">0.3869</td>
    </tr>
  </tbody>
</table>

<p>Table 2.6: Linear SVC</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Default</strong></td>
      <td style="text-align: center"><strong>0.6107</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">random_state = none, dual = False, tol = 1e-5</td>
      <td style="text-align: center">0.6107</td>
    </tr>
    <tr>
      <td style="text-align: center">random_state = 1, dual = True, tol = 1e-5</td>
      <td style="text-align: center">0.6107</td>
    </tr>
  </tbody>
</table>

<p>Table 2.7: Random Forest</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Parameters</th>
      <th style="text-align: center">Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Default</td>
      <td style="text-align: center">0.4919</td>
    </tr>
    <tr>
      <td style="text-align: center">n_estimators = 50</td>
      <td style="text-align: center">0.5348</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>n_estimators = 100</strong></td>
      <td style="text-align: center"><strong>0.5469</strong></td>
    </tr>
    <tr>
      <td style="text-align: center">n_estimators = 150</td>
      <td style="text-align: center">0.5463</td>
    </tr>
    <tr>
      <td style="text-align: center">n_estimators = 200</td>
      <td style="text-align: center">0.5225</td>
    </tr>
  </tbody>
</table>

<h3 id="data-processing-1">Data Processing</h3>

<p>From previous section: We’ve selected Linear SVC as our best performing classifier. We now use this classifier, its best performing parameters, and the bag of words feature extraction as our baseline for comparing different data processing techniques.</p>

<p>Some data processing steps we took that significantly increased our accuracy score consists of:</p>

<ul>
  <li>Removing urls, hashtags, and usernames</li>
  <li>Converting all characters to lowercase</li>
  <li>Removing numbers</li>
  <li>Removing punctuation</li>
</ul>

<p>The following tables display additional data processing techniques that we were able to try. Please note that each table also uses the best performing techniques from all previous tables.</p>

<p>Table 3.1: Reducing words to root form (Linear SVC, todo:params, bag of words)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">No Stemming/Lemmatization</th>
      <th style="text-align: center">Stemming</th>
      <th style="text-align: center">Lemmatization</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Score</strong></td>
      <td style="text-align: center">0.4529</td>
      <td style="text-align: center"><strong>0.5012</strong></td>
      <td style="text-align: center">0.4723</td>
    </tr>
  </tbody>
</table>

<p>Table 3.2: Emojis (Linear SVC, todo:params, bag of words, with stemming)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Keep Emojis</th>
      <th style="text-align: center">Removing Emojis</th>
      <th style="text-align: center">Replacing Emojis With Words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Score</strong></td>
      <td style="text-align: center">0.5012</td>
      <td style="text-align: center">0.5045</td>
      <td style="text-align: center"><strong>0.5532</strong></td>
    </tr>
  </tbody>
</table>

<p>Table 3.3: Stop Words (Linear SVC, todo:params, bag of words, with stemming, replacing emojis with words)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">No Stop Words</th>
      <th style="text-align: center">Stop Words (from NLTK)</th>
      <th style="text-align: center">Stop Words (custom list)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Score</strong></td>
      <td style="text-align: center">0.5532</td>
      <td style="text-align: center">0.5350</td>
      <td style="text-align: center"><strong>0.5718</strong></td>
    </tr>
  </tbody>
</table>

<p>Table 3.4: Slang Words (Linear SVC, todo:params, bag of words, with stemming, replacing emojis with words, and custom stop words list)</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">Keep Slang Words</th>
      <th style="text-align: center">Replace Specific Slang Words Before Stemming</th>
      <th style="text-align: center">Replace Specific Slang Words After Stemming</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Score</strong></td>
      <td style="text-align: center">0.5718</td>
      <td style="text-align: center"><strong>0.5731</strong></td>
      <td style="text-align: center">0.5715</td>
    </tr>
  </tbody>
</table>

<p>Table 3.5: Resampling</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">No Resampling</th>
      <th style="text-align: center">Upsampling</th>
      <th style="text-align: center">Downsampling</th>
      <th style="text-align: center">Upsampling and Downsampling</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><strong>Score</strong></td>
      <td style="text-align: center">0.5731</td>
      <td style="text-align: center">0.5655</td>
      <td style="text-align: center"><strong>0.6107</strong></td>
      <td style="text-align: center">0.5969</td>
    </tr>
  </tbody>
</table>

<hr />

<h1 id="analysis-of-the-results">Analysis of the Results</h1>

<p>In order to acquire meaningful results, we required a baseline model to try and improve upon. The model that we chose was provided for <a href="http://alt.qcri.org/semeval2018/index.php?id=tasks">SemEval-2018 Task 1</a>, which also focused on evaluating the affect in tweets. This task provided baseline code in Python and Java, but we opted to build our own from scratch to have a model that could be run solely in Python.</p>

<p>The baseline model correctly identified 52% of the tweets in the <a href="https://competitions.codalab.org/competitions/17751#results">test data provided with the task</a>. Our model attained a score of 61.1%, for a gain of over 9%. Our improvement over the baseline model can largely be attributed to the preprocessing steps that our model goes through before running machine learning models. In particular, the following steps helped prepare the data for the model.</p>

<ul>
  <li>
    <p>To give the model a more equal representation of the ordinal classes, we adjusted the number of tweets in each class by upsampling those with fewer tweets, and downsampling the ones with more. For example, in the training data set, the neutral and ‘-2’ classes have nearly three times as many tweets as the ‘-1’ class. When running the test set, the model skews heavily towards these classes with more tweets. Tweaking the samples helps to offset this, and gives something of a handicap to those classes with fewer tweets.</p>
  </li>
  <li>
    <p>Removing hashtags, URLs, and any other information containing symbols helps to sanitize the data. The model does not glean any useful information from these symbols, and including them only allows for possible confusion. For example, the model will not process “#happy” in the same way as the word ‘happy’, but the model needs to understand that their meanings are identical.</p>
  </li>
  <li>
    <p>Emojis can contain useful information relating to the poster’s mood, but it’s more useful to relate the emoji to a word that conveys the same emotion. A crying emoji should convey sadness, but the model will only be able to relate that to other tweets containing a crying emoji. If all of these emojis are replaced with a word that describes them, the model is potentially given access to a larger base of relevant tweets.</p>
  </li>
  <li>
    <p>Replacing certain abbreviations and slang words, such as ‘lol’ or ‘rofl’, also helps the model’s success rate. Substituting all of these similar abbreviations with their matching word (e.g. ‘laugh’) allows the model to essentially treat these words as synonyms, giving it a broader dataset to work with.</p>
  </li>
  <li>
    <p>Other “artifacts” in the dataset, such as multiple spaces between words, are also removed. This also helps to sanitize the data and removes any confusion in how individual words are delimited and processed by the model.</p>
  </li>
</ul>

<hr />

<h1 id="future-work">Future Work</h1>

<p>There were a few methods that we had hoped to research, but we had unfortunately ran out of time to both learn and implement them.</p>

<ul>
  <li>
    <p>More time to research preprocessing methods, as this seemed to have the largest effect on our success rate. In particular, finding a way to deal with spelling errors would likely help considerably, as this essentially gives the model more data to work with.</p>
  </li>
  <li>
    <p>Implementing methods to generate “fake” data based on the existing training data. This is something that works well in Computer Vision, and while it may be more difficult to generate new tweets than to slightly alter images, this would give the model more information to work with.</p>
  </li>
  <li>
    <p>The baseline model used a “multi-label Meka model” to classify the tweets, and achieved a score above 50% without any of the preprocessing we were able to implement. Before we had implemented our preprocessing steps, a lot of the models we tried were scoring considerably lower than the baseline score. It was also stated in the baseline task that the most successful participants used the baseline code instead of implementing their own. Given all of this, it would have been nice to see how our preprocessing steps would have improved upon the baseline code. Unfortunately, due to time constraints, it was much easier to build our own model in pure Python than to work with the provided WEKA package.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

  </div>

  
</article>

        </div>
      </div>
      <div class="footer">
    <nav class="navbar navbar-expand-lg justify-content-between">
        <ul class="navbar-nav social-nav">
            <li class="nav-item social">
                <a class="nav-link" href="https://github.com/sheldonfries" target="_blank"><i class="fab fa-github"></i></a>
            </li>
            <li class="nav-item social">
                <a class="nav-link" href="https://www.linkedin.com/in/sheldonfries/" target="_blank"><i class="fab fa-linkedin"></i></a>
            </li>
            <li class="nav-item social">
                <a class="nav-link" href="https://play.google.com/store/apps/developer?id=Sheldon+Fries&hl=en" target="_blank"><i class="fab fa-android"></i></a>
            </li>
            <li class="nav-item social">
                <a class="nav-link" href="mailto:sheldonfries@gmail.com" target="_blank"><i class="fas fa-envelope"></i></a>
            </li>
        </ul>
    </nav>
</div> 

  </body>
</html>